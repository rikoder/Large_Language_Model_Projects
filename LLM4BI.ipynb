{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rikoder/Large_Language_Model_Projects/blob/main/LLM4BI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRfU46i2Z4DU"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install --upgrade openai\n",
        "!pip install python-dotenv\n",
        "!pip install tiktoken\n",
        "!pip install gspread pandas\n",
        "!pip install gspread gspread_dataframe pandas\n",
        "%pip install --upgrade gspread #IMPORTANT\n",
        "!pip install pandasai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import openai\n",
        "import pandas as pd\n",
        "import tiktoken # https://github.com/openai/tiktoken\n",
        "import numpy as np\n",
        "import itertools\n",
        "import string\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "from datetime import datetime, timedelta\n",
        "from datetime import date\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "from google.colab import auth\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from pandasai import PandasAI\n",
        "from pandasai.llm.openai import OpenAI\n",
        "from sklearn import linear_model\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hmYvoPraEG0",
        "outputId": "606d0428-fdd0-4223-a961-b4f250b6bc06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"API KEY\"\n",
        "api_key =\"API KEY\""
      ],
      "metadata": {
        "id": "gmL6_FQ0FmiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Functions for Supply Chain Metrics and KPIs and OpenAI API call**"
      ],
      "metadata": {
        "id": "q5CtQYV56abM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ABC_classification(product_master, sale_data, period_months=12, A=0.7, B=0.9, C=1):\n",
        "  from datetime import datetime\n",
        "  from dateutil.relativedelta import relativedelta\n",
        "  # calculate the net invoice amount for each item for the specified period of analysis\n",
        "  date_formatted = (datetime.now() - relativedelta(months=period_months)).strftime('%Y-%m-%d')\n",
        "  product_sales = sale_data[sale_data['sale_order_date'] >= date_formatted].groupby('product_id')['net_invoice_amount'].sum().reset_index()\n",
        "  # Calculate the net invoice amount for each item\n",
        "  product_sales = product_sales.groupby('product_id')['net_invoice_amount'].sum().reset_index()\n",
        "  ##complete the list of products from product master as some product may be missing in product_sales\n",
        "  complete_product_sales = pd.merge(product_master[['product_id']], product_sales, on='product_id', how='left')\n",
        "  complete_product_sales['net_invoice_amount'].fillna(0, inplace=True)\n",
        "  # Sort the sales in descending order\n",
        "  complete_product_sales = complete_product_sales.sort_values(by='net_invoice_amount', ascending=False)\n",
        "  complete_product_sales['cum_invoice_amount'] = complete_product_sales['net_invoice_amount'].cumsum()\n",
        "  complete_product_sales['cum_sale_contribution'] = complete_product_sales['cum_invoice_amount'] / complete_product_sales['net_invoice_amount'].sum()\n",
        "  condition_A = (complete_product_sales['cum_sale_contribution'] <= A)\n",
        "  condition_B = (complete_product_sales['cum_sale_contribution'] > A) & (complete_product_sales['cum_sale_contribution'] <= B)\n",
        "  condition_C = (complete_product_sales['cum_sale_contribution'] > B) & (complete_product_sales['cum_sale_contribution'] <= C)\n",
        "  complete_product_sales['ABC_class'] = np.select([condition_A, condition_B, condition_C], ['A', 'B', 'C'], default='C')\n",
        "  # merge ABC classification back to product_master and fill missing values with 'C'\n",
        "  product_master = pd.merge(product_master, complete_product_sales[['product_id', 'ABC_class']], on='product_id', how='left')\n",
        "  product_master['ABC_class'].fillna('C', inplace=True)\n",
        "  sale_data = pd.merge(sale_data, product_master[['product_id', 'ABC_class']], on='product_id', how='left')\n",
        "  return product_master, sale_data\n",
        "\n",
        "def calculate_fill_rate(filtered_sale_data):\n",
        "    # calculating total order value\n",
        "    filtered_sale_data['total_order_value'] = filtered_sale_data['sale_order_qty'] * filtered_sale_data['price']\n",
        "    total_order_value = filtered_sale_data['total_order_value'].sum()\n",
        "    net_invoice_amount= filtered_sale_data['net_invoice_amount'].sum()\n",
        "    fill_rate = 100*net_invoice_amount/total_order_value\n",
        "    return fill_rate\n",
        "\n",
        "def read_data():\n",
        "  product_master = pd.read_csv('/content/drive/My Drive/supply_chain_gpt/product_master.csv')\n",
        "  sale_data = pd.read_csv('/content/drive/My Drive/supply_chain_gpt/sale_data.csv')\n",
        "  SOH_history = pd.read_csv('/content/drive/My Drive/supply_chain_gpt/SOH_history.csv')\n",
        "  distribution_master= pd.read_csv('/content/drive/My Drive/supply_chain_gpt/distribution_master.csv')\n",
        "  date_cols = [col for col in sale_data.columns if re.search('date', col, re.IGNORECASE)]\n",
        "  sale_data[date_cols] = sale_data[date_cols].apply(pd.to_datetime)\n",
        "  date_cols = [col for col in SOH_history.columns if re.search('date', col, re.IGNORECASE)]\n",
        "  SOH_history[date_cols] = SOH_history[date_cols].apply(pd.to_datetime)\n",
        "  return\n",
        "\n",
        "def get_embedding(text):\n",
        "    result = openai.Embedding.create(\n",
        "      model='text-embedding-ada-002',\n",
        "      input=text\n",
        "    )\n",
        "    return result[\"data\"][0][\"embedding\"]\n",
        "\n",
        "def vector_similarity(vec1,vec2):\n",
        "  return np.dot(np.array(vec1), np.array(vec2))\n",
        "\n",
        "def ask_gpt_3(question,scm_context):\n",
        "    #scm_context['embedding'] = scm_context['key_words'].apply(get_embedding)\n",
        "    prompt_embedding = get_embedding(question)\n",
        "    scm_context[\"prompt_similarity\"] = scm_context['embedding'].apply(lambda vector: vector_similarity(vector, prompt_embedding))\n",
        "    scm_context =scm_context.sort_values(by =\"prompt_similarity\", ascending = False)\n",
        "    top_context =scm_context[scm_context['key_words']!= 'Generic']\n",
        "    top_context = top_context[:2]\n",
        "    generic_row = scm_context[scm_context['key_words'] == 'Generic']\n",
        "    top_context = generic_row.append(top_context)\n",
        "    top_context = top_context['context'].tolist()\n",
        "    top_context = '\\n'.join(top_context)\n",
        "\n",
        "    prompt = f\"\"\"{top_context}\n",
        "              Question: give only python code using pandas. do not use python dictionaries.\n",
        "              call functions and make use of function.do not use read_and_convert().\n",
        "              do not give explainations starting with #, strictly use given context.\n",
        "              product master does not have  any date column so strictly do not filter product master on any type of date\n",
        "              strictly include python code current_date = pd.to_datetime('today')\n",
        "              strictly use only above mentioned dataframes and columns without removing _ in their names.make sure to check all variables are defined.{question}\n",
        "              Answer:\"\"\"\n",
        "\n",
        "    openai.api_key =api_key\n",
        "    prompt = (prompt)\n",
        "    response = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt= prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=1000,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "        )\n",
        "    output = response[\"choices\"][0][\"text\"]\n",
        "    code = output\n",
        "    lines = code.splitlines()\n",
        "    filtered_lines = [line for line in lines if not (line.startswith(\"#\") or \".csv\" in line or \"Error\" in line or \"error\" in line or \"assuming\" in line or \"use\" in line)]\n",
        "\n",
        "    lines = code.split(\"\\n\")\n",
        "    new_lines = []\n",
        "    for line in lines:\n",
        "        line = line.replace(\"<code>\", \"\")\n",
        "        line = line.replace(\"</code>\", \"\")\n",
        "        new_lines.append(line)\n",
        "    ### read all data###\n",
        "    new_code = \"\\n\".join(new_lines)\n",
        "    product_master = pd.read_csv('/content/drive/My Drive/supply_chain_gpt/product_master.csv')\n",
        "    sale_data = pd.read_csv('/content/drive/My Drive/supply_chain_gpt/sale_data.csv')\n",
        "    SOH_history = pd.read_csv('/content/drive/My Drive/supply_chain_gpt/SOH_history.csv')\n",
        "    distribution_master= pd.read_csv('/content/drive/My Drive/supply_chain_gpt/distribution_master.csv')\n",
        "    date_cols = [col for col in sale_data.columns if re.search('date', col, re.IGNORECASE)]\n",
        "    sale_data[date_cols] = sale_data[date_cols].apply(pd.to_datetime)\n",
        "    date_cols = [col for col in SOH_history.columns if re.search('date', col, re.IGNORECASE)]\n",
        "    SOH_history[date_cols] = SOH_history[date_cols].apply(pd.to_datetime)\n",
        "    new_code = \"\\n\".join(new_lines)\n",
        "    ################################\n",
        "    return print(new_code),print(exec(new_code))\n"
      ],
      "metadata": {
        "id": "M9nsY5gHFZ4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read and Preprocess Suppply Chain Data**"
      ],
      "metadata": {
        "id": "4pxWhp3q6yK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "product_master = pd.read_csv('/content/drive/My Drive/supply_chain_gpt/product_master.csv')\n",
        "sale_data = pd.read_csv('/content/drive/My Drive/supply_chain_gpt/sale_data.csv')\n",
        "SOH_history = pd.read_csv('/content/drive/My Drive/supply_chain_gpt/SOH_history.csv')\n",
        "distribution_master= pd.read_csv('/content/drive/My Drive/supply_chain_gpt/distribution_master.csv')\n",
        "date_cols = [col for col in sale_data.columns if re.search('date', col, re.IGNORECASE)]\n",
        "sale_data[date_cols] = sale_data[date_cols].apply(pd.to_datetime)\n",
        "date_cols = [col for col in SOH_history.columns if re.search('date', col, re.IGNORECASE)]\n",
        "SOH_history[date_cols] = SOH_history[date_cols].apply(pd.to_datetime)"
      ],
      "metadata": {
        "id": "eTNC10XKaJaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read Supply Chain Context, Prompts and get embeddings for Input into Large Language Model(GPT-3) from Google Sheet**"
      ],
      "metadata": {
        "id": "lYWNJqNDbSJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from google.auth import default\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "worksheet = gc.open('scm_domain_context').sheet1\n",
        "#get_all_values gives a list of rows\n",
        "rows = worksheet.get_all_values()\n",
        "df = pd.DataFrame(rows)\n",
        "df.columns = df.iloc[0]\n",
        "df = df.iloc[1:]\n",
        "scm_context = df"
      ],
      "metadata": {
        "id": "qZ6d7J9BbhKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scm_context['embedding'] = scm_context['key_words'].apply(get_embedding)"
      ],
      "metadata": {
        "id": "3yc4orIq9N7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question =\"lets think step by step, first do ABC classification and then Compare the fill rate for the top 10 B class products for the last 6 months to the same period last year and calculate the percentage change. and print percentage change\"\n",
        "ask_gpt_3(question,scm_context)"
      ],
      "metadata": {
        "id": "3zcWu_uCdxDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "177b42f7-5c62-49f8-892d-ccaf382139fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "# Import relativedelta from dateutil\n",
            "from dateutil.relativedelta import relativedelta\n",
            "# Get the current date\n",
            "current_date = pd.to_datetime('today')\n",
            "# Define the x period as 6 months\n",
            "x = relativedelta(months=6)\n",
            "# Calculate the start and end dates for x period ago\n",
            "start_date_x = current_date - x\n",
            "end_date_x = current_date\n",
            "# Calculate the start and end dates for same period last year\n",
            "start_date_ly = start_date_x - relativedelta(years=1)\n",
            "end_date_ly = end_date_x - relativedelta(years=1)\n",
            "\n",
            "# Calculate ABC classification\n",
            "product_master, sale_data = calculate_ABC_classification(product_master, sale_data, period_months=18, A=0.7, B=0.9, C=1)\n",
            "\n",
            "# Filter sale_data for last 6 months\n",
            "sale_data_x = sale_data[(sale_data['sale_order_date']>=start_date_x) & (sale_data['sale_order_date']<=end_date_x)]\n",
            "# Filter sale_data for same period last year\n",
            "sale_data_ly = sale_data[(sale_data['sale_order_date']>=start_date_ly) & (sale_data['sale_order_date']<=end_date_ly)]\n",
            "\n",
            "# Filter sale_data for B class products\n",
            "sale_data_x_B = sale_data_x[sale_data_x['ABC_class'] == 'B']\n",
            "sale_data_ly_B = sale_data_ly[sale_data_ly['ABC_class'] == 'B']\n",
            "\n",
            "# Get the top 10 B class products\n",
            "top_10_B_products_x = sale_data_x_B.groupby('product_id')['net_invoice_amount'].sum().reset_index().sort_values(by='net_invoice_amount',ascending=False).head(10)['product_id'].tolist()\n",
            "top_10_B_products_ly = sale_data_ly_B.groupby('product_id')['net_invoice_amount'].sum().reset_index().sort_values(by='net_invoice_amount',ascending=False).head(10)['product_id'].tolist()\n",
            "\n",
            "# Filter sale_data for top 10 B class products\n",
            "sale_data_x_top_10_B = sale_data_x_B[sale_data_x_B['product_id'].isin(top_10_B_products_x)]\n",
            "sale_data_ly_top_10_B = sale_data_ly_B[sale_data_ly_B['product_id'].isin(top_10_B_products_ly)]\n",
            "\n",
            "# Calculate fill rate for top 10 B class products\n",
            "fill_rate_x = calculate_fill_rate(sale_data_x_top_10_B)\n",
            "fill_rate_ly = calculate_fill_rate(sale_data_ly_top_10_B)\n",
            "\n",
            "# Calculate percentage change\n",
            "percentage_change = (fill_rate_x - fill_rate_ly) / fill_rate_ly * 100\n",
            "\n",
            "# Print percentage change\n",
            "print(percentage_change)\n",
            "3.551118066463456\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_list = '''\n",
        "what are current year to date revenues for DC DC-001. compare current year to date revenues with last year to date revenues for DC-001 and print the comparision\n",
        "lets think step by step, first do ABC classification and then Compare the fill rate for the top 10 B class products for the last 6 months to the same period last year and calculate the percentage change. and print percentage change.\n",
        "'''"
      ],
      "metadata": {
        "id": "-Zj1bwd3GikQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MUlraNu0HeBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HbDK4-7D7sEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sIOHu2GQbQCy"
      }
    }
  ]
}